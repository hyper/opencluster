
OpenCluster Load Balancing.

In order to keep the cluster functioning smoothly across the entire range of nodes, it needs to be able to 
balance the data (and presumably the load) amongst the different nodes.

  NOTE:  It is assumed that all the nodes in the cluster have equivilant processing and storage 
         capabilities.  If it is determined that there is a need to specify that some nodes in the 
         cluster are more capable than others, and give them more load, we can evaluate this 
         functionality, but it may be easier to have a cluster of similar spec machines.

         Technically, the specifications of the server would not be that fantastic anyway, so a 
         cluster of various machines from low-end to high-end should assume that the lowest spec 
         machine in the minimum required for the amount of functionality required.


When the first node in the cluster starts up, it becomes responsible for all the buckets, and there 
are no backups.   If there is only one node in the cluster, and that node goes offline, then the 
cluster is gone also.

A single node can still function just as well as a cluster, and from a client perspective would 
provide full functionality (aside from replication of data).

When a second node joins the cluster, the first one will give half of its buckets to the secondary 
server to be primary, and will tell the second node to also be a backup of the other buckets.  This 
results in both servers having a copy of all the buckets in the cluster.

  example of a single server...

  +--------------------------+
  |                          |
  |  Server 1                |
  |    Bucket #1  - primary  |
  |    Bucket #2  - primary  |
  |    Bucket #3  - primary  |
  |    Bucket #4  - primary  |
  |    Bucket #5  - primary  |
  |    Bucket #6  - primary  |
  |    Bucket #7  - primary  |
  |    Bucket #8  - primary  |
  |    Bucket #9  - primary  |
  |    Bucket #10 - primary  |
  |    Bucket #11 - primary  |
  |    Bucket #12 - primary  |
  |    Bucket #13 - primary  |
  |    Bucket #14 - primary  |
  |    Bucket #15 - primary  |
  |    Bucket #16 - primary  |
  |                          |
  +--------------------------+

  then when a second node joins the first.. the buckets are split over the two servers 


  +--------------------------+  +--------------------------+
  |                          |  |                          |
  |  Server A                |  |  Server B                |
  |    Bucket #1  - primary  |  |    Bucket #1  - backup   |
  |    Bucket #2  - backup   |  |    Bucket #2  - primary  |
  |    Bucket #3  - primary  |  |    Bucket #3  - backup   |
  |    Bucket #4  - backup   |  |    Bucket #4  - primary  |
  |    Bucket #5  - primary  |  |    Bucket #5  - backup   |
  |    Bucket #6  - backup   |  |    Bucket #6  - primary  |
  |    Bucket #7  - primary  |  |    Bucket #7  - backup   |
  |    Bucket #8  - backup   |  |    Bucket #8  - primary  |
  |    Bucket #9  - primary  |  |    Bucket #9  - backup   |
  |    Bucket #10 - backup   |  |    Bucket #10 - primary  |
  |    Bucket #11 - primary  |  |    Bucket #11 - backup   |
  |    Bucket #12 - backup   |  |    Bucket #12 - primary  |
  |    Bucket #13 - primary  |  |    Bucket #13 - backup   |
  |    Bucket #14 - backup   |  |    Bucket #14 - primary  |
  |    Bucket #15 - primary  |  |    Bucket #15 - backup   |
  |    Bucket #16 - backup   |  |    Bucket #16 - primary  |
  |                          |  |                          |
  +--------------------------+  +--------------------------+

      NOTE: although this diagram shows a pattern between primary 
            and backup balancing, the server would actually 
            transfer the buckets in random order.

Also note that, if the backup buckets are kept in sync with the primary buckets, then a server can 
go offline quickly and orderly by simply telling the servers containing the backup buckets, that 
they are now primary.

The way the bucket migration works, is that when a node joins another server, it establishes an 
event on that server that fires every so often.  

So lets go back to the state of thing when Server B joined the cluster that was previously only the 
single Server A.


  +--------------------------+  +--------------------------+
  |                          |  |                          |
  |  Server A                |  |  Server B                |
  |    Bucket #1  - primary  |  |    Bucket #1  -          |
  |    Bucket #2  - primary  |  |    Bucket #2  -          |
  |    Bucket #3  - primary  |  |    Bucket #3  -          |
  |    Bucket #4  - primary  |  |    Bucket #4  -          |
  |    Bucket #5  - primary  |  |    Bucket #5  -          |
  |    Bucket #6  - primary  |  |    Bucket #6  -          |
  |    Bucket #7  - primary  |  |    Bucket #7  -          |
  |    Bucket #8  - primary  |  |    Bucket #8  -          |
  |    Bucket #9  - primary  |  |    Bucket #9  -          |
  |    Bucket #10 - primary  |  |    Bucket #10 -          |
  |    Bucket #11 - primary  |  |    Bucket #11 -          |
  |    Bucket #12 - primary  |  |    Bucket #12 -          |
  |    Bucket #13 - primary  |  |    Bucket #13 -          |
  |    Bucket #14 - primary  |  |    Bucket #14 -          |
  |    Bucket #15 - primary  |  |    Bucket #15 -          |
  |    Bucket #16 - primary  |  |    Bucket #16 -          |
  |                          |  |                          |
  +--------------------------+  +--------------------------+

At this point, both Server A and Server B would have a LoadBalancing event fire every so often 
(more often when the cluster is unbalanced, less often when things are balanced).  This event will 
query the other node, saying "How much load do you have?" and the node will respond with something 
like "12 primary buckets, 4 secondary buckets".  The node will compare that reply with what it 
currently has and will decide if it wants to push a bucket to the other server or not.

  NOTE: New nodes are not able to request buckets.  They just receive them when other nodes 
        determine that it should be given one.  This could be a result of balancing, or it could be 
        a result of that other server being shutdown and it must send the buckets somewhere.

So, Server A sends a request asking Server B for its load levels, and Server B, at this point, Sends 
back that it is not currently holding any primary or secondary buckets.  Server A will then look at 
how many Primary and Secondary buckets it has, determins that it has more primaries than 
secondaries, and so decides to send a primary to the other server.

When deciding to send a bucket, it first decides whether to send a primary or a backup, but then it 
will choose the bucket randomly among that category.

Server A sends a message to B saying "I'm going to be giving you Bucket #16, be prepared for it".  
At this point Server B would normally respond with an "OK, start sending", but if it is already in 
the process of being given a bucket from another node, it can say "Sorry, Bucket transfer already in 
progress".  In this case though, there is only the two nodes, so it will be responding positively.  

Server A then starts sending all the data contained in Bucket #16.  It will send it item by item, 
trying not to overload either server with a sudden rush of traffic, because while this 
synchronisation is going on, it will also need to service requests from clients, including requests 
for that bucket being transferred.

  NOTE: Server A is still responsible for that bucket while it is being transferred and will still 
        be updating the data and responding to requests.   Each bucket will maintain a list of items 
        that have been modified.  The sync process will initially fill that list with all the items, 
        and remove them as it sync's them.  When updates occur, that item is just re-added to the 
        list.

While the sync is going on, it will not trigger any more LoadBalancing events.  In other words, it 
will only be attempting to balance one bucket at a time.

When Server A has managed to send ALL the items belonging to Bucket #16 to Server B, the following 
things happen:
  1.  Server A tells Server B "You are now the primary node responsible for Bucket #16"
  2.  Server A marks its own copy of the bucket as a backup.
  3.  Server A removes Server B as a backup of that bucket, so that it doesn't sync data to it 
      anymore.
  4.  Server B marks its copy of the bucket as the primary.
  5.  Server A sends a message to all its connected clients (and nodes) saying that it is a backup 
      provider of Bucket #16.
  6.  Server B sends a message to all its connected clients (and nodes) saying that it is a primary 
      provider of Bucket #16.
  7.  Server B makes a note that when changes are made on that bucket, the information also needs to 
      be sent to Server A, because it is a backup of the bucket.

At this point, all the clients now know that if it is accessing data from that bucket, it now needs 
to go to Server B.  If a client is a little slow to get the message though, and it does send a 
request to Server A, Server A will respond saying something like "I am not responsible for 
Bucket #16, please ask Server B".   This allows for the migration to occur safely without extensive 
co-ordination with the nodes and clients.

Now we have:

  +--------------------------+  +--------------------------+
  |                          |  |                          |
  |  Server A                |  |  Server B                |
  |    Bucket #1  - primary  |  |    Bucket #1  -          |
  |    Bucket #2  - primary  |  |    Bucket #2  -          |
  |    Bucket #3  - primary  |  |    Bucket #3  -          |
  |    Bucket #4  - primary  |  |    Bucket #4  -          |
  |    Bucket #5  - primary  |  |    Bucket #5  -          |
  |    Bucket #6  - primary  |  |    Bucket #6  -          |
  |    Bucket #7  - primary  |  |    Bucket #7  -          |
  |    Bucket #8  - primary  |  |    Bucket #8  -          |
  |    Bucket #9  - primary  |  |    Bucket #9  -          |
  |    Bucket #10 - primary  |  |    Bucket #10 -          |
  |    Bucket #11 - primary  |  |    Bucket #11 -          |
  |    Bucket #12 - primary  |  |    Bucket #12 -          |
  |    Bucket #13 - primary  |  |    Bucket #13 -          |
  |    Bucket #14 - primary  |  |    Bucket #14 -          |
  |    Bucket #15 - primary  |  |    Bucket #15 -          |
  |    Bucket #16 - backup   |  |    Bucket #16 - primary  |
  |                          |  |                          |
  +--------------------------+  +--------------------------+


So now that the sync and migration of the bucket has completed, regular LoadBalancing events will 
continue to be fired.  

Server B will ask Server A for its LoadLevel's but will see that A has more buckets, and therefore 
B should not send any to A.  Remember, a node cannot request buckets, it can only give them away.

Server A, when asking Server B for LoadLevels will now be told, "I have 1 primary and 0 secondary 
buckets".   Server A will try and keep a balance on the other node between primary and backup 
buckets, and sees that there are more primaries than there are backups, so it decides to make it be 
a backup of one of the primary buckets.  The actual algorithm that does this calculation is a little 
more complex than that, but not by much.

Server A randomly descides to send bucket #9.  This is a little different to the previous case, 
because it is merely going to mark Server B as a backup for the bucket and the usual sync process 
will take place.   Server B will send out a notice to all its connected clients saying that it is a 
backup node of bucket #9.

Server A will add all the items in Bucket #9 to the sync queue, and will start sending that out to 
Server B.  While it is doing that, updates will probably occur on Bucket #9 and those items will 
simply be added to the sync-queue as normal.   The only difference now between this operation and 
normal sync operations to a backup server, is that the first time the sync data is caught up, it 
will trigger LoadBalancing events to start firing again.   Remember, we only want to blast out a 
sync of one bucket at a time to a server.  Normal sync operations on other buckets would still 
continue though, regardless of the state of the bucket being migrated.  A message from A to B 
stating "I have finished migrating bucket #9 to you", so that Server B can know that it can accept 
Buckets from other nodes again.

At the begining of this process, we can assume:

  +--------------------------+  +--------------------------+
  |                          |  |                          |
  |  Server A                |  |  Server B                |
  |    Bucket #1  - primary  |  |    Bucket #1  -          |
  |    Bucket #2  - primary  |  |    Bucket #2  -          |
  |    Bucket #3  - primary  |  |    Bucket #3  -          |
  |    Bucket #4  - primary  |  |    Bucket #4  -          |
  |    Bucket #5  - primary  |  |    Bucket #5  -          |
  |    Bucket #6  - primary  |  |    Bucket #6  -          |
  |    Bucket #7  - primary  |  |    Bucket #7  -          |
  |    Bucket #8  - primary  |  |    Bucket #8  -          |
  |    Bucket #9  - primary  |  |    Bucket #9  - backup   |
  |    Bucket #10 - primary  |  |    Bucket #10  -         |
  |    Bucket #11 - primary  |  |    Bucket #11 -          |
  |    Bucket #12 - primary  |  |    Bucket #12 -          |
  |    Bucket #13 - primary  |  |    Bucket #13 -          |
  |    Bucket #14 - primary  |  |    Bucket #14 -          |
  |    Bucket #15 - primary  |  |    Bucket #15 -          |
  |    Bucket #16 - backup   |  |    Bucket #16 - primary  |
  |                          |  |                          |
  +--------------------------+  +--------------------------+

The same process will continue, until the buckets on the two servers are balanced.  The algorithm to 
decide needs to ensure that it never tries to switch between two servers that already have the 
bucket.   So in the above example where Server A is the primary for Bucket #9 and Server B is the 
secondary, it would never try and send Bucket #9 to B as a primary.   This can be more evident when 
there are a large number of nodes, and some start dissapearing and buckets have to get 
re-introduced.

In the end, we could end up with something like:

  +--------------------------+  +--------------------------+
  |                          |  |                          |
  |  Server A                |  |  Server B                |
  |    Bucket #1  - backup   |  |    Bucket #1  - primary  |
  |    Bucket #2  - primary  |  |    Bucket #2  - backup   |
  |    Bucket #3  - backup   |  |    Bucket #3  - primary  |
  |    Bucket #4  - primary  |  |    Bucket #4  - backup   |
  |    Bucket #5  - backup   |  |    Bucket #5  - primary  |
  |    Bucket #6  - backup   |  |    Bucket #6  - primary  |
  |    Bucket #7  - primary  |  |    Bucket #7  - backup   |
  |    Bucket #8  - primary  |  |    Bucket #8  - backup   |
  |    Bucket #9  - primary  |  |    Bucket #9  - backup   |
  |    Bucket #10 - backup   |  |    Bucket #10 - primary  |
  |    Bucket #11 - primary  |  |    Bucket #11 - backup   |
  |    Bucket #12 - backup   |  |    Bucket #12 - primary  |
  |    Bucket #13 - primary  |  |    Bucket #13 - backup   |
  |    Bucket #14 - backup   |  |    Bucket #14 - primary  |
  |    Bucket #15 - primary  |  |    Bucket #15 - backup   |
  |    Bucket #16 - backup   |  |    Bucket #16 - primary  |
  |                          |  |                          |
  +--------------------------+  +--------------------------+


This can get further complicated, when a third server joins the cluster.

  +--------------------------+  +--------------------------+  +--------------------------+
  |                          |  |                          |  |                          |
  |  Server A                |  |  Server B                |  |  Server C                |
  |    Bucket #1  - backup   |  |    Bucket #1  - primary  |  |    Bucket #1  -          |
  |    Bucket #2  - primary  |  |    Bucket #2  - backup   |  |    Bucket #2  -          |
  |    Bucket #3  - backup   |  |    Bucket #3  - primary  |  |    Bucket #3  -          |
  |    Bucket #4  - primary  |  |    Bucket #4  - backup   |  |    Bucket #4  -          |
  |    Bucket #5  - backup   |  |    Bucket #5  - primary  |  |    Bucket #5  -          |
  |    Bucket #6  - backup   |  |    Bucket #6  - primary  |  |    Bucket #6  -          |
  |    Bucket #7  - primary  |  |    Bucket #7  - backup   |  |    Bucket #7  -          |
  |    Bucket #8  - primary  |  |    Bucket #8  - backup   |  |    Bucket #8  -          |
  |    Bucket #9  - primary  |  |    Bucket #9  - backup   |  |    Bucket #9  -          |
  |    Bucket #10 - backup   |  |    Bucket #10 - primary  |  |    Bucket #10 -          |
  |    Bucket #11 - primary  |  |    Bucket #11 - backup   |  |    Bucket #11 -          |
  |    Bucket #12 - backup   |  |    Bucket #12 - primary  |  |    Bucket #12 -          |
  |    Bucket #13 - primary  |  |    Bucket #13 - backup   |  |    Bucket #13 -          |
  |    Bucket #14 - backup   |  |    Bucket #14 - primary  |  |    Bucket #14 -          |
  |    Bucket #15 - primary  |  |    Bucket #15 - backup   |  |    Bucket #15 -          |
  |    Bucket #16 - backup   |  |    Bucket #16 - primary  |  |    Bucket #16 -          |
  |                          |  |                          |  |                          |
  +--------------------------+  +--------------------------+  +--------------------------+

The first server to fire a LoadBalancing event against Server C will see that it has less buckets 
than the other, and will start sending buckets to it.   Remember that only one Bucket can be 
transferred to the node at a time.

One extra thing to note here, is that by default the servers will be setup to only keep one backup of each bucket.  So if a primary bucket is being migrated to Server C, then the current backup node will be told that it can release the bucket.

We can use Bucket #1 as an example.  Here we have Server A being the backup, and B being the primary.  If B is wanting to move that bucket to Server C, it would start the process as in the previous examples.   It will start sending the data to C, and when finished, C will become the primary, B will become the backup, and A will be told that it can drop the bucket entirely.

So we end up with:

  +--------------------------+  +--------------------------+  +--------------------------+
  |                          |  |                          |  |                          |
  |  Server A                |  |  Server B                |  |  Server C                |
  |    Bucket #1  -          |  |    Bucket #1  - backup   |  |    Bucket #1  - primary  |
  |    Bucket #2  - primary  |  |    Bucket #2  - backup   |  |    Bucket #2  -          |
  |    Bucket #3  - backup   |  |    Bucket #3  - primary  |  |    Bucket #3  -          |
  |    Bucket #4  - primary  |  |    Bucket #4  - backup   |  |    Bucket #4  -          |
  |    Bucket #5  - backup   |  |    Bucket #5  - primary  |  |    Bucket #5  -          |
  |    Bucket #6  - backup   |  |    Bucket #6  - primary  |  |    Bucket #6  -          |
  |    Bucket #7  - primary  |  |    Bucket #7  - backup   |  |    Bucket #7  -          |
  |    Bucket #8  - primary  |  |    Bucket #8  - backup   |  |    Bucket #8  -          |
  |    Bucket #9  - primary  |  |    Bucket #9  - backup   |  |    Bucket #9  -          |
  |    Bucket #10 - backup   |  |    Bucket #10 - primary  |  |    Bucket #10 -          |
  |    Bucket #11 - primary  |  |    Bucket #11 - backup   |  |    Bucket #11 -          |
  |    Bucket #12 - backup   |  |    Bucket #12 - primary  |  |    Bucket #12 -          |
  |    Bucket #13 - primary  |  |    Bucket #13 - backup   |  |    Bucket #13 -          |
  |    Bucket #14 - backup   |  |    Bucket #14 - primary  |  |    Bucket #14 -          |
  |    Bucket #15 - primary  |  |    Bucket #15 - backup   |  |    Bucket #15 -          |
  |    Bucket #16 - backup   |  |    Bucket #16 - primary  |  |    Bucket #16 -          |
  |                          |  |                          |  |                          |
  +--------------------------+  +--------------------------+  +--------------------------+

If by a quirk of fate, Server B happens to be rather busy, and A manages to send half its buckets 
to C, then it is still ok, because B will stop being a backup for some of the buckets.  So it 
should still end up being rather balanced.

This same scenario will scale up to many more nodes.

This situation doesn't mean that there can be only a maximum of 16 nodes, because when a server gets 
to the point where it has only 4 buckets, it will split those 4 buckets into smaller buckets.  
In the process of splitting and informing the other nodes of the changed hashmasks, that will 
trigger all the other nodes in the cluster to also split their own buckets.   It may take a few 
cycles for the splits to complete across the cluster, but during that time, all the hashes will 
still resolve to the same nodes, so nothing should be disturbed.   After the split, those 4 buckets 
on the server would now be 64 smaller buckets.  It can continue to load balance them in the exact 
same way as we did before with only 16 buckets.  Those original 16 buckets on the cluster, have now 
become 256 buckets (stored twice).

The same again can happen when there are even more nodes in the cluster.  It will split to have a 
total of 4096 primary buckets.


You may have noticed that up until now, Nodes have only been transferring buckets that they are 
primarily responsible for.  When determining which buckets to send to the other server, it first 
looks for any primary buckets that do not have a backup.  If all primary buckets have a backup, then 
it will count how many primary buckets and how many backup buckets it has, and whichever pile is 
greater, it will transfer one (and it doesn't matter which one).

The difference is, that if transferring a primary bucket, it can decide whether or not to make it a 
backup or primary on the other side.  But when transferring a bucket that is already a backup, it 
can only make it a backup on the other side.  The node that already has the primary bucket is the 
only one that can demote and promote.






NODES LEAVING THE CLUSTER

So far this document has explained how a cluster grows, but it is just as important to shrink 
gracefully also.

The number of buckets wont reduce.  Once the buckets are split, they stay split.  But thats ok, 
because it doesnt really matter if there is 4096 buckets spread over only a few nodes.

When the cluster is generally in sync, there are normally two copies of each bucket.  If a server 
needs  to shutdown (gracefully), it should be able to quickly release the buckets it has, and the 
backup copies become primary.   

If the nodes are badly out of sync when a shutdown is needed, it would need to wait until the sync 
is completed.




HANDLING A DEAD NODE

Handling the situation of a dead node is rather tricky, but important.

A heartbeat is sent between servers.  Also, a Bucket_SYNC heartbeat is sent from the primary to the 
backup bucket every second.  This is used to determine how current the bucket is.  Part of this SYNC 
command is the number of items that are currently pending synchronisation.  This is useful for the 
receiving server to know how complete the bucket transfer is.

If the Node with the backup bucket does not receive the SYNC heartbeat several times in a row, and 
has not received any other data from the server (its possible the server has been sending over 
capacity, and the heartbeats are still on the pipe), then the backup bucket will go into a PENDING 
mode.  It doesn't actively do anything, but it will set that flag.  If it does get data from that 
server, it will clear the flag and everything will go back to normal.   

The clients generally know the primary and backup servers for each bucket.  If a client is unable to 
get a responce within a certain timeout from the primary server, it will close the connection to the 
primary, and will ask the backup server for the data instead.   If the backup server is NOT in 
PENDING mode, then it will simply reply with a message saying "Not responsible for that bucket... 
try this other server instead".   But if it IS in that PENDING state, then it will assume that if it 
cant see the other server, and the clients cant see the other server, then the other server must be 
dead.  It will disconnect any remaining connection to the other server, and promote its own copy of 
the bucket to primary.  It will also start looking for other servers to transfer backup buckets to.

  NOTE: If two nodes are comparing load levels, and they are the same, a node can still send a 
        bucket to another node if there is no backup bucket for that bucket.

If the node that died comes back, with intact buckets, it will find that those buckets already exist 
on the cluster, and it will need to delete them.  The tricky part is determining which server is 
correct, and which server is not.   



