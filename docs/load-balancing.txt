
OpenCluster Load Balancing.


In order to keep the cluster functioning smoothly across the entire range of nodes, it needs to be able to balance the data (and presumably the load) amongst the different nodes.

  NOTE:  It is assumed that all the nodes in the cluster have equivilant processing and 
         storage capabilities.  If it is determined that there is a need to specify that 
         some nodes in the cluster are more capable than others, and give them more load, 
         we can evaluate this functionality, but it may be easier to have a cluster of 
         similar spec machines.

         Technically, the specifications of the server would not be that fantastic anyway, 
         so a cluster of various machines from low-end to high-end should assume that the 
         lowest spec machine is the minimum required for the amount of functionality 
         required.


When the first node in the cluster starts up, it becomes responsible for all the buckets, and there are no backups.   If there is only one node in the cluster, and that node goes offline, then the cluster (and all its data) is gone also.

A single node can still function just as well as a multi-node cluster, and from a client perspective would provide full functionality (aside from replication of data).

When a second node joins the cluster, the first one will ultimately give half of its buckets to the secondary server to be primary, and will tell the second node to also be a backup of the other buckets.  This results in both servers having a copy of all the buckets in the cluster.

  Example of a single server with 16 buckets...

 +--------+ 
 | NODE-A | 
 | #0 - P | 
 | #1 - P | 
 | #2 - P | 
 | #3 - P | 
 | #4 - P | 
 | #5 - P | 
 | #6 - P | 
 | #7 - P | 
 | #8 - P | 
 | #9 - P | 
 | #A - P | 
 | #B - P | 
 | #C - P | 
 | #D - P | 
 | #E - P | 
 | #F - P | 
 +--------+ 
   16+0=16      

  then when a second node joins the first.. the buckets are split over the two servers 

 +--------+ +--------+
 | NODE-A | | NODE-B |
 | #0 - S | | #0 - P |
 | #1 - S | | #1 - P |
 | #2 - S | | #2 - P |
 | #3 - S | | #3 - P |
 | #4 - S | | #4 - P |
 | #5 - S | | #5 - P |
 | #6 - S | | #6 - P |
 | #7 - S | | #7 - P |
 | #8 - P | | #8 - S |
 | #9 - P | | #9 - S |
 | #A - P | | #A - S |
 | #B - P | | #B - S |
 | #C - P | | #C - S |
 | #D - P | | #D - S |
 | #E - P | | #E - S |
 | #F - P | | #F - S |
 +--------+ +--------+
   8+8=16     8+8=16       


Migration Rules (in priority order):

  1. Do not promote a bucket on target node, if that results in source node 
     having more secondary than primary buckets.

  2. Promote a backup bucket if target has more secondary than primary buckets.

  3. A Node can only transfer or receive one bucket at a time.

  4. If target node is not full, send it any bucket that does not have a 
     backup node.

  5. Node can transfer either a primary or secondary bucket to any other node, 
     whatever the source has most of.

  6. Transfer a bucket only if target has less than the ideal number of 
     buckets (based on how many nodes are in the cluster).

  7. Only transfer a primary bucket if source has at least one more primary 
     bucket than target.

  8. Do not transfer a bucket if source node does not have an ideal number of 
     buckets already.




Note that, if the backup buckets are kept in sync with the primary buckets, then a server can go offline quickly and orderly by simply telling the servers containing the backup buckets, that they are now primary.

When a node joins another server, it establishes an event on that server that fires every so often.  

So lets go back to the state of things when NODE-B joined the cluster that was previously only the single NODE-A.

 +--------+ +--------+
 | NODE-A | | NODE-B |
 | #0 - P | | #0 -   |
 | #1 - P | | #1 -   |
 | #2 - P | | #2 -   |
 | #3 - P | | #3 -   |
 | #4 - P | | #4 -   |
 | #5 - P | | #5 -   |
 | #6 - P | | #6 -   |
 | #7 - P | | #7 -   |
 | #8 - P | | #8 -   |
 | #9 - P | | #9 -   |
 | #A - P | | #A -   |
 | #B - P | | #B -   |
 | #C - P | | #C -   |
 | #D - P | | #D -   |
 | #E - P | | #E -   |
 | #F - P | | #F -   |
 +--------+ +--------+
   16+0=16    0+0=0       

At this point, both NODE-A and NODE-B would have a LoadBalancing event fire every so often (more often when the cluster is unbalanced, less often when things are balanced).  This event will query the other node, saying "How much load do you have?" and the node will respond with something like "12 primary buckets, 4 secondary buckets" (as an example).  The node will compare that reply with what load it currently has and will decide if it wants to push a bucket to the other server or not.

  NOTE: New nodes are not able to request buckets.  They just receive them 
        when other nodes determine that it should be given one.  This could be 
        a result of balancing, or it could be a result of that other server 
        being shutdown and it must send the buckets somewhere.

So, NODE-A sends a request asking NODE-B for its load levels, and NODE-B, at this point, sends back that it is not currently holding any primary or secondary buckets.  NODE-A will then look at how many Primary and Secondary buckets it has, determines that it has more buckets than B, and so decides to send send one to the other server.

  NOTE: If a node receives a request to take over a bucket, it should do its 
        best to accomodate.  It may be that the other server is shutting down 
        and is offloading buckets.  So the receiving server should always 
        accept new buckets, UNLESS it is already in the process of receiving 
        another bucket from some other node.  A node can only receive one 
        bucket at a time.

Servers nodes can send either primary or secondary buckets to another server, depending on which it has most of.  When a node is transferring a bucket to another node, it must tell the existing partner of the bucket where the bucket is going to.

NODE-A sends a message to NODE-B saying "I'm going to be giving you Bucket #0, be prepared for it".  At this point NODE-B would normally respond with an "OK, start sending bucket #0", but if it is already in the process of being given a bucket from another node, it can say "Sorry, Bucket transfer already in progress".  In this case though, there is only the two nodes, so it will be responding positively.  

Since NODE-A has buckets that do not have a backup copy, it will attempt to send one of those first.  These buckets will have to be primary, so it will send them as a new bucket, to be secondary on the target node (NODE-B).  Once all the buckets have backup copies, the migration behaviour is different.

NODE-A then starts sending all the data contained in Bucket #0.  It will send it item by item (but probably 10 at a time or so), trying not to overload either server with a sudden rush of traffic, because during this synchronisation, it will also need to service requests from clients, including requests for that bucket being transferred.

  NOTE: NODE-A is still responsible for that bucket while it is being 
        transferred and will still be updating the data and responding to 
        requests.   Each bucket will maintain a list of items that have been 
        modified.  The sync process will initially fill that list with all the 
        items, and remove them as it sync's them.  When updates occur, that 
        item is just re-added to the list.

  WARNING: There is a real concern that on busy clusters, a bucket will never 
           be able to finish migrating because new updates will arrive at a 
           rate equal or more than the rate at which it can send the data to 
           the new server.  We may need to implement a timeout of some sort, 
           that if the migration takes longer than a certain time, then it 
           stops accepting updates for that bucket until the migration is 
           complete.

While the sync is going on, it will not trigger any more LoadBalancing events (or, if it does, they will be ignored).  In other words, it will only be attempting to balance one bucket at a time.

When NODE-A has managed to send ALL the items belonging to Bucket #0 to NODE-B, the following 
things happen:

 1.  NODE-A tells NODE-B "Synchronisation is complete, you are now a backup 
     for Bucket #0"

 2.  NODE-A marks NODE-B as a backup of that bucket, and will continue sending 
     sync info to that node when contents of the bucket is changed.

 3.  NODE-B sends a message to all its connected clients (and nodes) saying 
     that it is now a backup provider of Bucket #0.

 4.  NODE-A and B will start doing LoadBalancing events again.


Now we have:

 +--------+ +--------+
 | NODE-A | | NODE-B |
 | #0 - P | | #0 - S |
 | #1 - P | | #1 -   |
 | #2 - P | | #2 -   |
 | #3 - P | | #3 -   |
 | #4 - P | | #4 -   |
 | #5 - P | | #5 -   |
 | #6 - P | | #6 -   |
 | #7 - P | | #7 -   |
 | #8 - P | | #8 -   |
 | #9 - P | | #9 -   |
 | #A - P | | #A -   |
 | #B - P | | #B -   |
 | #C - P | | #C -   |
 | #D - P | | #D -   |
 | #E - P | | #E -   |
 | #F - P | | #F -   |
 +--------+ +--------+
   16+0=16    0+1=1       


So now that the sync and migration of the bucket has completed, regular LoadBalancing events will continue to be fired.  

NODE-B will ask NODE-A for its LoadLevel's but will see that A has more buckets, and therefore 
B should not send any to A.  Remember, a node cannot request buckets, it can only give them away.

  NOTE: If a server node knows that it has less that the ideal number of 
        buckets, it doesn't need to send LoadLevel requests to other nodes.  
        However, it should send PING's instead to be sure that it still has 
        valid communications with all the other nodes.

NODE-A, when asking NODE-B for LoadLevels will now be told, "I have 0 primary and 1 secondary buckets".  NODE-A sees that it's total number of buckets exceeds B's, so decides to send it another bucket.

NODE-A at this point still has plenty of buckets that do not have backup copies, so it will continue to send those as a priority.

NODE-A sends bucket #1 to NODE-B using the same process as above.

At the end of this step, we can assume:

 +--------+ +--------+
 | NODE-A | | NODE-B |
 | #0 - P | | #0 - S |
 | #1 - P | | #1 - S |
 | #2 - P | | #2 -   |
 | #3 - P | | #3 -   |
 | #4 - P | | #4 -   |
 | #5 - P | | #5 -   |
 | #6 - P | | #6 -   |
 | #7 - P | | #7 -   |
 | #8 - P | | #8 -   |
 | #9 - P | | #9 -   |
 | #A - P | | #A -   |
 | #B - P | | #B -   |
 | #C - P | | #C -   |
 | #D - P | | #D -   |
 | #E - P | | #E -   |
 | #F - P | | #F -   |
 +--------+ +--------+
   16+0=16    0+2=2       

Now, when NODE-A gets a LoadLevel from NODE-B, it sees that B has more secondaries than it has primaries, so it attempts to promote a bucket they both have.  In this case, Bucket #0.   This process is quite fast and straight forward.

  NOTE: Clients can request events to be sent when activity occurs on a stored 
        key.  When promoting a bucket, those events need to then start coming 
        from the primary node.  The client will need to ensure that it is 
        connected to all the nodes, in order to keep getting those events.

When the promotion occurs, both nodes will send updates to their clients indicating what level they maintain.  At the end of this step, we will have:

 +--------+ +--------+
 | NODE-A | | NODE-B |
 | #0 - S | | #0 - P |
 | #1 - P | | #1 - S |
 | #2 - P | | #2 -   |
 | #3 - P | | #3 -   |
 | #4 - P | | #4 -   |
 | #5 - P | | #5 -   |
 | #6 - P | | #6 -   |
 | #7 - P | | #7 -   |
 | #8 - P | | #8 -   |
 | #9 - P | | #9 -   |
 | #A - P | | #A -   |
 | #B - P | | #B -   |
 | #C - P | | #C -   |
 | #D - P | | #D -   |
 | #E - P | | #E -   |
 | #F - P | | #F -   |
 +--------+ +--------+
   15+1=16    1+1=2       


The same process will continue, until the number of buckets on the two servers are balanced.  
In the end, we could end up with something like:

 +--------+ +--------+
 | NODE-A | | NODE-B |
 | #0 - S | | #0 - P |
 | #1 - S | | #1 - P |
 | #2 - S | | #2 - P |
 | #3 - S | | #3 - P |
 | #4 - S | | #4 - P |
 | #5 - S | | #5 - P |
 | #6 - S | | #6 - P |
 | #7 - S | | #7 - P |
 | #8 - P | | #8 - S |
 | #9 - P | | #9 - S |
 | #A - P | | #A - S |
 | #B - P | | #B - S |
 | #C - P | | #C - S |
 | #D - P | | #D - S |
 | #E - P | | #E - S |
 | #F - P | | #F - S |
 +--------+ +--------+
   8+8=16     8+8=16       

Note that the cluster is now balanced.  

The LoadBalancing events will continue to be fired from each one, but at a much reduced rate.  When a new node connects to the cluster, or another node exits, it will send these requests at a faster rate.

This can get further complicated, when a third server joins the cluster.

 +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C |
 | #0 - S | | #0 - P | | #0 -   |
 | #1 - S | | #1 - P | | #1 -   |
 | #2 - S | | #2 - P | | #2 -   |
 | #3 - S | | #3 - P | | #3 -   |
 | #4 - S | | #4 - P | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   |
 | #8 - P | | #8 - S | | #8 -   |
 | #9 - P | | #9 - S | | #9 -   |
 | #A - P | | #A - S | | #A -   |
 | #B - P | | #B - S | | #B -   |
 | #C - P | | #C - S | | #C -   |
 | #D - P | | #D - S | | #D -   |
 | #E - P | | #E - S | | #E -   |
 | #F - P | | #F - S | | #F -   |
 +--------+ +--------+ +--------+
   8+8=16     8+8=16     0+0=0       

The first server to fire a LoadBalancing event against NODE-C will see that it has less buckets than the other, and will start sending buckets to it.   Remember that only one Bucket can be transferred to the node at a time.

So if we assume that NODE-A happens to communicate with NODE-C first, it will see that NODE-C has less than ideal buckets, and will decide to send one.  NODE-A knows that it has the same number of primary and secondary, so it decides to send a secondary bucket to NODE-C, in this case, Bucket #0.

Since this is a bit different to the previous examples which were sending buckets that did not have a backup copy, we will go through this process step by step.

NODE-A sends a message to NODE-C saying "I am sending Bucket #0 to you, and it will be a backup copy from NODE-B"

NODE-C will send a reply acknowledging this and will prepare to receive the data for bucket #0.  NODE-A will then start sending all the data and event subscriptions to NODE-C.  Since NODE-C is going to be a backup, it will not actually action any of the event subscriptions, it will just remember them in case it must become primary due to a failure on NODE-B.

During this transfer process, which might take a few seconds to a few minutes depending on the amount of data, NODE-B will continue to handle client requests, and will continue to send updates to NODE-A, which is still the actual backup node at this point.

When all the data has been sent:

  1.  NODE-A will send a message to NODE-C saying "Data transfer complete, you are now backup node for Bucket #0, NODE-B is the primary"

  2.  NODE-A will stop accepting SYNC data from NODE-B for this bucket.

  3.  NODE-A will send a message to NODE-B saying that NODE-C is now the backup node for bucket #0.  

  4.  NODE-B will send a message to NODE-C saying "I will be sending you SYNC data for Bucket #0"

  4.  NODE-A will send a message to all its clients and nodes saying "I am no longer holding bucket #0"

  5.  NODE-C will send a message to all its clients and nodes saying "I am a backup node for bucket #0"

So now we end up with:

 +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C |
 | #0 -   | | #0 - P | | #0 - S*|
 | #1 - S | | #1 - P | | #1 -   |
 | #2 - S | | #2 - P | | #2 -   |
 | #3 - S | | #3 - P | | #3 -   |
 | #4 - S | | #4 - P | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   |
 | #8 - P | | #8 - S | | #8 -   |
 | #9 - P | | #9 - S | | #9 -   |
 | #A - P | | #A - S | | #A -   |
 | #B - P | | #B - S | | #B -   |
 | #C - P | | #C - S | | #C -   |
 | #D - P | | #D - S | | #D -   |
 | #E - P | | #E - S | | #E -   |
 | #F - P | | #F - S | | #F -   |
 +--------+ +--------+ +--------+
   8+7=15     8+8=16     0+1=1       

Note that at this point, Bucket #0 on NODE-C has been marked as "Last Transfer" and will not be chosen to be sent to a different node.  This insures that we do not get a situation where the number of buckets do not split evenly over the nodes, we do not want a bucket moving around the system.  The current migration algorithm does not require this functionality anymore, but it does help in reducing the number of times a particular bucket is moves around the system.

So now NODE-C has a bucket which came from NODE-A.   NODE-B sends a LoadLevel request and also decides to send a bucket, also a secondary and chooses Bucket #8.

Now we have:

 +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C |
 | #0 -   | | #0 - P | | #0 - S |
 | #1 - S | | #1 - P | | #1 -   |
 | #2 - S | | #2 - P | | #2 -   |
 | #3 - S | | #3 - P | | #3 -   |
 | #4 - S | | #4 - P | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   |
 | #8 - P | | #8 -   | | #8 - S*|
 | #9 - P | | #9 - S | | #9 -   |
 | #A - P | | #A - S | | #A -   |
 | #B - P | | #B - S | | #B -   |
 | #C - P | | #C - S | | #C -   |
 | #D - P | | #D - S | | #D -   |
 | #E - P | | #E - S | | #E -   |
 | #F - P | | #F - S | | #F -   |
 +--------+ +--------+ +--------+
   8+7=15     8+7=15     0+2=2       

Note that now Bucket #8 is marked as the "Last Transfer" bucket and not #0.  Only one bucket per node is marked this way.

As soon as a migration is complete, a Node will normally immediately send a LoadLevel request to try and move more buckets quickly.  NODE-B sees that NODE-C has more secondaries than primaries, so it decides to switch Bucket #0.  This works the same way as previously described before, so we'll end up with:

 +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C |
 | #0 -   | | #0 - S | | #0 - P |
 | #1 - S | | #1 - P | | #1 -   |
 | #2 - S | | #2 - P | | #2 -   |
 | #3 - S | | #3 - P | | #3 -   |
 | #4 - S | | #4 - P | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   |
 | #8 - P | | #8 -   | | #8 - S*|
 | #9 - P | | #9 - S | | #9 -   |
 | #A - P | | #A - S | | #A -   |
 | #B - P | | #B - S | | #B -   |
 | #C - P | | #C - S | | #C -   |
 | #D - P | | #D - S | | #D -   |
 | #E - P | | #E - S | | #E -   |
 | #F - P | | #F - S | | #F -   |
 +--------+ +--------+ +--------+
   8+7=15     7+8=15     1+1=2       

Since this was a switch, we dont really need to change the "Last Transfer" flag.

So now NODE-A manages to get the next LoadLevel responce from NODE-C and it decides to send a bucket because NODE-C has less than the ideal number of buckets.  Since NODE-A has more primary than secondary buckets, it decides to send a primary.   

Sending a primary bucket is slightly different to what we've covered already, so we'll describe it is detail.

First it will send a message to NODE-C saying "I am going to send you primary bucket #9", and NODE-C will respond appropriately.  NODE-A will then begin sending all the data and event subscription details to NODE-C.

When NODE-A has sent all it can:

  1.  NODE-A will send a message to NODE-C saying "Transfer complete for 
      Bucket #9, accept client requests now".

  2.  NODE-C will accept client requests for Bucket #9 but will not send SYNC 
      data to the backup node yet.

  3.  NODE-A will stop accepting client requests, and will forward them on to 
      NODE-C.

  4.  NODE-A will send a message to all clients and nodes saying "I do not 
      have Bucket #9 anymore, NODE-C does".

  5.  NODE-A will send a message to NODE-B saying "I will not be sending you 
      updates anymore for Bucket #9, NODE-C is the primary"

  6.  NODE-B will acknowledge the handover.

  7.  When NODE-B acknowledges, NODE-A will send a final message to NODE-C 
      saying "Bucket #9 is all yours"

  8.  NODE-C will begin sending SYNC data to NODE-B which is the backup node 
      for the bucket (including the items that were not sent during the handover).

At this point, NODE-C is the primary and all communication for that bucket will go through it.

 +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C |
 | #0 -   | | #0 - S | | #0 - P |
 | #1 - S | | #1 - P | | #1 -   |
 | #2 - S | | #2 - P | | #2 -   |
 | #3 - S | | #3 - P | | #3 -   |
 | #4 - S | | #4 - P | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   |
 | #8 - P | | #8 -   | | #8 - S |
 | #9 -   | | #9 - S | | #9 - P*|
 | #A - P | | #A - S | | #A -   |
 | #B - P | | #B - S | | #B -   |
 | #C - P | | #C - S | | #C -   |
 | #D - P | | #D - S | | #D -   |
 | #E - P | | #E - S | | #E -   |
 | #F - P | | #F - S | | #F -   |
 +--------+ +--------+ +--------+
   7+7=14     7+8=15     2+1=3       

So if we continue processing the bucket migration, we would end up with something like:

Note that the Ideal number of buckets for each node is 10.  This is 16 (number of buckets in the mask), times 2 (number of backups for each bucket), divided by 3 (the number of nodes in the cluster), and rounded down.   This means that each node in the cluster should have 10 or more buckets (either primary or secondary).

 +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C |
 | #0 -   | | #0 - S | | #0 - P |
 | #1 -   | | #1 - P | | #1 - S |
 | #2 -   | | #2 - P | | #2 - S |
 | #3 - S | | #3 -   | | #3 - P |
 | #4 - S | | #4 - P | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   |
 | #8 - P | | #8 -   | | #8 - S |
 | #9 -   | | #9 - S | | #9 - P |
 | #A -   | | #A - S | | #A - P |
 | #B -   | | #B - S | | #B - P |
 | #C - P | | #C -   | | #C - S |
 | #D - P | | #D -   | | #D - S |
 | #E - P | | #E -   | | #E - S*|
 | #F - P | | #F - S | | #F -   |
 +--------+ +--------+ +--------+
   5+5=10     6+5=11     5+6=11       

Now we have a completely balanced cluster, with an even mix of primary and secondary buckets on all three nodes.  The process will continue when other nodes join the cluster.

Things only get tricky when a server attempts to splits its buckets, while another server is sending a bucket to yet another server.  When that occurs, the transfer is stopped, and restarted, the data already sent discarded.  Then, rebalancing would continue with a new number of buckets.

This situation doesn't mean that there can be only a maximum of 16 nodes, because when a server gets to the point where it has only 4 buckets (total), it will split those 4 buckets into smaller buckets.  In the process of splitting and informing the other nodes of the changed hashmasks, that will trigger all the other nodes in the cluster to also split their own buckets.   It may take a few cycles for the splits to complete across the cluster, but during that time, all the hashes will still resolve to the same nodes, so nothing should be disturbed.   After the split, those 4 buckets on the server would now be 64 smaller buckets.  It can continue to load balance them in the exact same way as we did before with only 16 buckets.  Those original 16 buckets on the cluster, have now become 256 buckets (stored twice, so actually 512).

The same again can happen when there are even more nodes in the cluster.  It will split to have a total of 4096 primary buckets.


WHAT HAPPENS IF A SPLIT HAPPENS WHILE TRANSFERRING A BUCKET?

A split would occur if another node sends a HASHMASK command that has a new mask.  This will require the node to split its own buckets.  When this happens a migration in progress is no longer valid, so it will be cancelled.  The target of the migration will delete the contents that it has already received, and it would likely be receiving messages shortly telling requiring it to split also.

If a receiving node (of a migration) needs to split, then it will send a message to the source node telling it that it is cancelling the transfer.

Once the migration is cancelled, normal loadlevel activity will commence and the new buckets will start migrating out to servers with less load.



NODES LEAVING THE CLUSTER

So far this document has explained how a cluster grows, but it is just as important to shrink gracefully also.

The number of buckets wont reduce.  Once the buckets are split, they stay split.  But thats ok, because it doesnt really matter if there is 4096 buckets spread over only a few nodes.

When the cluster is generally in sync, there are normally two copies of each bucket.  If a server needs  to shutdown (gracefully), it should be able to quickly release the buckets it has, and the backup copies become primary.  That node would then try to find other nodes to create a backup bucket on.

If the nodes are badly out of sync when a shutdown is needed, it would need to wait until the sync is completed.




HANDLING A DEAD NODE

Handling the situation of a dead node is rather tricky, but important.

A heartbeat is sent between servers.  Also, a Bucket_SYNC heartbeat is sent from the primary to the backup bucket every second.  This is used to determine how current the bucket is.  Part of this SYNC command is the number of items that are currently pending synchronisation.  This is useful for the receiving server to know how complete the bucket transfer is.

If the Node with the backup bucket does not receive the SYNC heartbeat several times in a row, and has not received any other data from the server (its possible the server has been sending over capacity, and the heartbeats are still on the pipe), then the backup bucket will go into a PENDING mode.  It doesn't actively do anything, but it will set that flag.  If it does get data from that server, it will clear the flag and everything will go back to normal.   

The clients generally know the primary and backup servers for each bucket.  If a client is unable to get a responce within a certain timeout from the primary server, it will close the connection to the primary, and will ask the backup server for the data instead.   If the backup server is NOT in PENDING mode, then it will simply reply with a message saying "Not responsible for that bucket... try this other server instead".   But if it IS in that PENDING state, then it will assume that if it cant see the other server, and the clients cant see the other server, then the other server must be dead.  It will disconnect any remaining connection to the other server, and promote its own copy of the bucket to primary.  It will also start looking for other servers to transfer backup buckets to.

  NOTE: If two nodes are comparing load levels, and they have the same number 
        of buckets, a node can still send a bucket to another node if there is 
        no backup bucket for that bucket.   This is only likely to occur if 
        several nodes go offline and leaves the cluster in a group, with intact 
        buckets, it will find that those buckets already exist on the cluster, 
        and it will need to delete them.  The tricky part is determining which 
        server is correct, and which server is not.   



PROBLEMS NOT YET DEALT WITH.

  1.  What happens if a server goes offline suddenly, and it was the only node in the cluster 
      holding a certain bucket (because the initial sync was still occuring).

  2.  What happens if two nodes go down at the same time (maybe a data center failure), and those 
      two servers were the only ones on the cluster responsible for a couple of buckets.  What will 
      trigger the other nodes to re-create those missing buckets?

  3.  When a network error happens and neither node is able to determine which one is responsible 
      for a bucket, and has had local clients updating both of them.  They both have new data.  How 
      can those broken buckets be merged?


CONTINUING THE CLUSTER WITH MORE NODES

Where we left off.

 +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C |
 | #0 -   | | #0 - S | | #0 - P |
 | #1 -   | | #1 - P | | #1 - S |
 | #2 -   | | #2 - P | | #2 - S |
 | #3 - S | | #3 -   | | #3 - P |
 | #4 - S | | #4 - P | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   |
 | #8 - P | | #8 -   | | #8 - S |
 | #9 -   | | #9 - S | | #9 - P |
 | #A -   | | #A - S | | #A - P |
 | #B -   | | #B - S | | #B - P |
 | #C - P | | #C -   | | #C - S |
 | #D - P | | #D -   | | #D - S |
 | #E - P | | #E -   | | #E - S*|
 | #F - P | | #F - S | | #F -   |
 +--------+ +--------+ +--------+
   5+5=10     6+5=11     5+6=11       

Add a node:

 +--------+ +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C | | NODE-D |
 | #0 -   | | #0 - S | | #0 - P | | #0 -   |
 | #1 -   | | #1 - P | | #1 - S | | #1 -   |
 | #2 -   | | #2 - P | | #2 - S | | #2 -   |
 | #3 - S | | #3 -   | | #3 - P | | #3 -   |
 | #4 - S | | #4 - P | | #4 -   | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   | | #7 -   |
 | #8 - P | | #8 -   | | #8 - S | | #8 -   |
 | #9 -   | | #9 - S | | #9 - P | | #9 -   |
 | #A -   | | #A - S | | #A - P | | #A -   |
 | #B -   | | #B - S | | #B - P | | #B -   |
 | #C - P | | #C -   | | #C - S | | #C -   |
 | #D - P | | #D -   | | #D - S | | #D -   |
 | #E - P | | #E -   | | #E - S*| | #E -   |
 | #F - P | | #F - S | | #F -   | | #F -   |
 +--------+ +--------+ +--------+ +--------+
   5+5=10     6+5=11     5+6=11     0+0=0      

Now that we have 4 nodes, the Ideal bucket level is 8.

And we handle all the loadlevel events that fire off.

 +--------+ +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C | | NODE-D |
 | #0 -   | | #0 -   | | #0 - P | | #0 - S |
 | #1 -   | | #1 -   | | #1 - S | | #1 - P |
 | #2 -   | | #2 -   | | #2 - S | | #2 - P |
 | #3 -   | | #3 -   | | #3 - P | | #3 - S |
 | #4 - S | | #4 - P | | #4 -   | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   | | #7 -   |
 | #8 -   | | #8 -   | | #8 - S | | #8 - P |
 | #9 -   | | #9 - S | | #9 -   | | #9 - P*|
 | #A -   | | #A - S | | #A - P | | #A -   |
 | #B -   | | #B - S | | #B - P | | #B -   |
 | #C - P | | #C -   | | #C -   | | #C - S |
 | #D - P | | #D -   | | #D -   | | #D - S |
 | #E - P | | #E -   | | #E - S*| | #E -   |
 | #F - P | | #F - S | | #F -   | | #F -   |
 +--------+ +--------+ +--------+ +--------+
   4+4=8      4+4=8      4+4=8      4+4=8      


If we add another node (the ideal becomes 6):

 +--------+ +--------+ +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C | | NODE-D | | NODE-E |
 | #0 -   | | #0 -   | | #0 - P | | #0 - S | | #0 -   |
 | #1 -   | | #1 -   | | #1 - S | | #1 - P | | #1 -   |
 | #2 -   | | #2 -   | | #2 - S | | #2 - P | | #2 -   |
 | #3 -   | | #3 -   | | #3 - P | | #3 - S | | #3 -   |
 | #4 - S | | #4 - P | | #4 -   | | #4 -   | | #4 -   |
 | #5 - S | | #5 - P | | #5 -   | | #5 -   | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   | | #6 -   | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   | | #7 -   | | #7 -   |
 | #8 -   | | #8 -   | | #8 - S | | #8 - P | | #8 -   |
 | #9 -   | | #9 - S | | #9 -   | | #9 - P*| | #9 -   |
 | #A -   | | #A - S | | #A - P | | #A -   | | #A -   |
 | #B -   | | #B - S | | #B - P | | #B -   | | #B -   |
 | #C - P | | #C -   | | #C -   | | #C - S | | #C -   |
 | #D - P | | #D -   | | #D -   | | #D - S | | #D -   |
 | #E - P | | #E -   | | #E - S*| | #E -   | | #E -   |
 | #F - P | | #F - S | | #F -   | | #F -   | | #F -   |
 +--------+ +--------+ +--------+ +--------+ +--------+
   4+4=8      4+4=8      4+4=8      4+4=8      0+0=0

And load level it up... we get to a point where....      

 +--------+ +--------+ +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C | | NODE-D | | NODE-E |
 | #0 -   | | #0 -   | | #0 -   | | #0 - S | | #0 - P |
 | #1 -   | | #1 -   | | #1 -   | | #1 - P | | #1 - S*|
 | #2 -   | | #2 -   | | #2 - S | | #2 - P | | #2 -   |
 | #3 -   | | #3 -   | | #3 - P | | #3 - S | | #3 -   |
 | #4 -   | | #4 - P | | #4 -   | | #4 -   | | #4 - S |
 | #5 - S | | #5 -   | | #5 -   | | #5 -   | | #5 - P |
 | #6 - S | | #6 - P | | #6 -   | | #6 -   | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   | | #7 -   | | #7 -   |
 | #8 -   | | #8 -   | | #8 - S | | #8 - P | | #8 -   |
 | #9 -   | | #9 -   | | #9 -   | | #9 - P*| | #9 - S |
 | #A -   | | #A - S | | #A - P | | #A -   | | #A -   |
 | #B -   | | #B - S | | #B - P | | #B -   | | #B -   |
 | #C -   | | #C -   | | #C -   | | #C - S | | #C - P |
 | #D - P | | #D -   | | #D -   | | #D - S | | #D -   |
 | #E - P | | #E -   | | #E - S*| | #E -   | | #E -   |
 | #F - P | | #F - S | | #F -   | | #F -   | | #F -   |
 +--------+ +--------+ +--------+ +--------+ +--------+
   3+3=6      3+3=6      3+3=6      4+4=8      3+3=6

Note that NODE-D has 8 buckets, when all the others have 6?   We are in this state, because all the nodes except this one now has their ideal amount.  The current algorithm does not correct this, because it can become a little more complicated by adding special rules for these end cases.  As an example, you might think it relatively safe to send a bucket if the node has two or more buckets than the other node, even if the other node has an ideal amount.  However, this could result in an inefficient initial migration as one node can end up sending too many buckets to another node, which then need to be sent out to other nodes again.

We want the migration process to be efficient to reduce load on the cluster and maintain responsiveness.  Once a cluster is balanced, there shouldn't be any more migration, so we need to get to that state as quickly as possible.


If we add one more node, the Ideal number of buckets per node becomes 5.

 +--------+ +--------+ +--------+ +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C | | NODE-D | | NODE-E | | NODE-F |
 | #0 -   | | #0 -   | | #0 -   | | #0 - S | | #0 - P | | #0 -   |
 | #1 -   | | #1 -   | | #1 -   | | #1 - P | | #1 - S*| | #1 -   |
 | #2 -   | | #2 -   | | #2 - S | | #2 - P | | #2 -   | | #2 -   |
 | #3 -   | | #3 -   | | #3 - P | | #3 - S | | #3 -   | | #3 -   |
 | #4 -   | | #4 - P | | #4 -   | | #4 -   | | #4 - S | | #4 -   |
 | #5 - S | | #5 -   | | #5 -   | | #5 -   | | #5 - P | | #5 -   |
 | #6 - S | | #6 - P | | #6 -   | | #6 -   | | #6 -   | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   | | #7 -   | | #7 -   | | #7 -   |
 | #8 -   | | #8 -   | | #8 - S | | #8 - P | | #8 -   | | #8 -   |
 | #9 -   | | #9 -   | | #9 -   | | #9 - P*| | #9 - S | | #9 -   |
 | #A -   | | #A - S | | #A - P | | #A -   | | #A -   | | #A -   |
 | #B -   | | #B - S | | #B - P | | #B -   | | #B -   | | #B -   |
 | #C -   | | #C -   | | #C -   | | #C - S | | #C - P | | #C -   |
 | #D - P | | #D -   | | #D -   | | #D - S | | #D -   | | #D -   |
 | #E - P | | #E -   | | #E - S*| | #E -   | | #E -   | | #E -   |
 | #F - P | | #F - S | | #F -   | | #F -   | | #F -   | | #F -   |
 +--------+ +--------+ +--------+ +--------+ +--------+ +--------+
   3+3=6      3+3=6      3+3=6      4+4=8      3+3=6      0+0=0

We process the loadbalancing and we get.

 +--------+ +--------+ +--------+ +--------+ +--------+ +--------+
 | NODE-A | | NODE-B | | NODE-C | | NODE-D | | NODE-E | | NODE-F |
 | #0 -   | | #0 -   | | #0 -   | | #0 -   | | #0 - P | | #0 - S |
 | #1 -   | | #1 -   | | #1 -   | | #1 - P | | #1 - S*| | #1 -   |
 | #2 -   | | #2 -   | | #2 -   | | #2 - S | | #2 -   | | #2 - P |
 | #3 -   | | #3 -   | | #3 - P | | #3 -   | | #3 -   | | #3 - S |
 | #4 -   | | #4 - P | | #4 -   | | #4 -   | | #4 - S | | #4 -   |
 | #5 -   | | #5 -   | | #5 -   | | #5 -   | | #5 - P | | #5 - S |
 | #6 - S | | #6 - P | | #6 -   | | #6 -   | | #6 -   | | #6 -   |
 | #7 - S | | #7 - P | | #7 -   | | #7 -   | | #7 -   | | #7 -   |
 | #8 -   | | #8 -   | | #8 - S | | #8 - P | | #8 -   | | #8 -   |
 | #9 -   | | #9 -   | | #9 -   | | #9 - P*| | #9 - S | | #9 -   |
 | #A -   | | #A -   | | #A - P | | #A -   | | #A -   | | #A - S |
 | #B -   | | #B - S | | #B - P | | #B -   | | #B -   | | #B -   |
 | #C -   | | #C -   | | #C -   | | #C - S | | #C - P | | #C -   |
 | #D - P | | #D -   | | #D -   | | #D - S | | #D -   | | #D -   |
 | #E - P | | #E -   | | #E - S*| | #E -   | | #E -   | | #E -   |
 | #F - P | | #F - S | | #F -   | | #F -   | | #F -   | | #F -   |
 +--------+ +--------+ +--------+ +--------+ +--------+ +--------+
   3+2=5      3+2=5      3+2=5      3+3=6      3+3=6      1+4=5

We are now fairly balanced, however, the new NODE-F is a bit unbalanced as far as primary and secondaries go, and it cannot really do any better because anything we do will just reduce the other nodes from being ideal as well.   The cutoff from splitting the buckets was 4 buckets.  This was chosen fairly arbitrarily because it hopefully represented a minimum of two primary and two secondary.  However, after looking at this result, it might be better if we have a minimum of 6 buckets.  By having more buckets to deal with, we have more things we can move around the cluster to keep things in balance.

In reality, the code is developed using 16 primary buckets to start with to make it easier to view the state of the cluster and fix bugs, but ideally we probably want to start the system with 256 primary buckets (512 in total) to reduce migration hassles when starting up with more than 6 nodes.  The kind of projects that would benefit from OpenCluster are probably intending to use 8 or more nodes, but unlikely to use more than 32, so 256 is probably a more realistic number of buckets to split the cluster into.


