tasks to do

work items:
* establish the chunk memory on startup.
* setup a timeout on read events from the client so that we can drop the connection if no data is
  received within a certain time.
* when starting up the server, drop (or ignore) client connections until it has finished establishing a stable 
  connection to the cluster (or assumed it is the only one in the cluster).



See the issues list:
https://github.com/hyper/opencluster/issues

* Add logging to a file, preferably on its own thread.

* figure out what to do when a cluster member drops out, is assumed dead, and then comes back.  Its buckets are now 
  invalid.


* since most of the client connections will have the same event-timeout, we can increase performance 
  by setting up a common timout value and use that with the events.  This will be useful if there 
  are a large number of clients, but will not give much benefit if there are not a lot of 
  connections.

* When shutting down, if it is not able to offload buckets to another server, it will write them out 
  to disk, and reload them when started again.  This way, the last node in a cluster can save and 
  restore the data... potentially offloading it to other nodes when the cluster is fully started up.



Performance

Each bucket has a backup copy on another node.  We need to be able to send updates to that other node for that bucket. The original design was that when items get updated, their details get added to a queue that then gets processed, and sent to the backup node.  However, that might mean that a lot of tree lookups are occuring.  So instead we need to send the data to the other server while we have it on hand.  Therefore, we should send that message immediately.  Additionally, when when we are migrating a bucket to a new node, we dont want to send the entire bucket at once, because that can flood the connection.  We need to be able to send it in chunks.   We were going to use a queue for that too, but then we still have to do a LOT of tree lookups to find the data.  What we need to do instead, is every entry in the tree has a sync-bit.   Whenever a full bucket transfer is needed, a master sync-bit is set, and all the objects in the tree that dont have that bit set, need to be transferred.  Every time we get enough responses back from the remote node and we need to send more out, we will iterate through the maps and trees and send out another bunch.  The map tree items will need to have a sync-bit as well, so that we can know which ones we can skip or not.

